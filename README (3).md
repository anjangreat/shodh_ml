# üè¶ Loan Approval Optimization ‚Äî Deep Learning & Offline Reinforcement Learning

This repository provides a complete end-to-end pipeline for **loan approval optimization**, integrating both **Deep Learning (DL)** and **Offline Reinforcement Learning (RL)** techniques.

The primary objective is to help a fintech institution **maximize profit** while **minimizing loan defaults** by learning from historical data and optimizing its approval strategy.

---

## üß≠ Project Overview

The workflow is organized into four major tasks:

1. **Task 1 ‚Äî Exploratory Data Analysis (EDA) & Preprocessing**
   Data cleaning, feature engineering, and dataset preparation.
2. **Task 2 ‚Äî Deep Learning Model (PyTorch MLP)**
   Supervised learning model to predict default probability.
3. **Task 3 ‚Äî Offline Reinforcement Learning (CQL)**
   RL-based agent that learns profit-maximizing loan approval policies using offline data.
4. **Task 4 ‚Äî Comparative Analysis & Insights**
   Evaluation of DL (AUC/F1) vs. RL (Estimated Policy Value), policy comparison, and future recommendations.

All experiments can be reproduced locally (Python **3.11** recommended) or on **Google Colab**.

---

## üóÅ Folder Structure

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ accepted_2007_to_2018Q4.csv.gz      # Raw dataset (optional)
‚îÇ   ‚îî‚îÄ‚îÄ loan_clean_subset.csv                # Cleaned dataset (from Task 1)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Task1_EDA_Preprocess.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Task2_DL_Predictive_Model.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Task3_Offline_RL_CQL.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Task4_Analysis_Comparison.ipynb      # Combined analysis notebook
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ model_mlp_default_risk/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pytorch_mlp.pt                   # Saved DL model weights
‚îÇ   ‚îî‚îÄ‚îÄ offline_rl_cql/
‚îÇ       ‚îú‚îÄ‚îÄ cql_discrete_model.d3            # Trained RL policy
‚îÇ       ‚îî‚îÄ‚îÄ preprocess.joblib                # Preprocessor for inference
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

> üí° **Note:** If the raw dataset is unavailable, you can still run the pipeline using `data/loan_clean_subset.csv` generated during Task 1.

---

## ‚öôÔ∏è Environment Setup

### üß¨ Option A ‚Äî Conda (Recommended)

> ‚ö†Ô∏è Use **Python 3.11** ‚Äî newer versions (e.g., 3.13) may break compatibility with RL and TensorFlow dependencies.

```bash
# From the repository root
conda create -n loan-rl python=3.11 -y
conda activate loan-rl

# Install dependencies
python -m pip install -U pip
pip install -r requirements.txt

# (Optional) Add Jupyter kernel
python -m ipykernel install --user --name loan-rl --display-name "Python 3.11 (loan-rl)"

# Launch Jupyter Notebook
jupyter notebook
# Then in Jupyter: Kernel ‚Üí Change Kernel ‚Üí "Python 3.11 (loan-rl)"
```

---

## üóÉÔ∏è Data Files

Place the following files inside the **`data/`** directory:

* **Preferred:** `loan_clean_subset.csv`
  ‚Äì Cleaned dataset generated after preprocessing
  ‚Äì Includes features and target column (`0 = Fully Paid`, `1 = Defaulted`)

* **Optional:** `accepted_2007_to_2018Q4.csv.gz`
  ‚Äì Raw dataset; Task 1 will automatically generate `loan_clean_subset.csv`

> Ensure that the columns **`loan_amnt`** and **`int_rate`** are present ‚Äî both are essential for RL reward computation.

---

## üöÄ How to Run the Pipeline

### üîπ Step 1 ‚Äî EDA & Preprocessing

Open **`notebooks/Task1_EDA_Preprocess.ipynb`** ‚Üí **Run All**
This notebook:

* Loads raw data (if available)
* Cleans and preprocesses features
* Saves `data/loan_clean_subset.csv`

**Output:** `data/loan_clean_subset.csv`

---

### üîπ Step 2 ‚Äî Deep Learning Model (DL)

Open **`notebooks/Task2_DL_Predictive_Model.ipynb`** ‚Üí **Run All**
This notebook:

* Loads the cleaned dataset
* Builds preprocessing and training pipelines (imputation + OHE + scaling)
* Trains a **PyTorch MLP** for default prediction
* Evaluates using **AUC** and **F1-Score**

**Outputs:**

* `models/model_mlp_default_risk/pytorch_mlp.pt`
* Printed metrics: AUC, F1, classification report, confusion matrix

---

### üîπ Step 3 ‚Äî Offline Reinforcement Learning (CQL)

Open **`notebooks/Task3_Offline_RL_CQL.ipynb`** ‚Üí **Run All**
This notebook:

* Converts data into a **bandit-style MDP** (actions = {approve, deny})
* Trains a **Conservative Q-Learning (CQL)** agent using **d3rlpy**
* Computes **Estimated Policy Value (EPV)** on the test set

**Outputs:**

* `models/offline_rl_cql/cql_discrete_model.d3`
* `models/offline_rl_cql/preprocess.joblib`
* Printed metrics: EPV, approval rate, approval breakdown

---

### üîπ Step 4 ‚Äî Analysis & Comparison

Open **`notebooks/Task4_Analysis_Comparison.ipynb`** ‚Üí **Run All**
This notebook:

* Reevaluates DL metrics (AUC, F1)
* Computes RL EPV and (optionally) FQE
* Displays cases where DL and RL disagree
* Generates the **Final Summary Report**

**Outputs:**

* Metrics summary (AUC, F1, EPV, approval rate)
* Policy disagreement table
* Final one-page report

---

## üíª Command-Line Quickstart (Optional)

Run all major tasks in one go:

```bash
# Ensure cleaned dataset exists
python notebooks/Task4_Analysis_Comparison.ipynb
```

This script will:

* Train and evaluate the DL model (AUC/F1)
* Train and evaluate the RL agent (EPV)
* Show policy disagreements and final results

---

## ü§ñ Troubleshooting Guide

| **Issue**                                            | **Solution**                                                               |
| ---------------------------------------------------- | -------------------------------------------------------------------------- |
| TensorFlow/d3rlpy incompatibility (Python 3.13)      | Use **Python 3.11** as shown in setup instructions.                        |
| `ModuleNotFoundError: gymnasium.wrappers.time_limit` | Install `gymnasium[classic-control]==0.29.1`.                              |
| RL fit() signature mismatch                          | Use positional form: `algo.fit(train_dataset, 100_000)` (already applied). |
| GPU not detected                                     | CPU works fine; ensure CUDA + PyTorch GPU build for acceleration.          |
| Missing `loan_amnt` or `int_rate` columns            | Re-run Task 1 to regenerate the cleaned dataset.                           |

---

## üìä Expected Outputs

| **Model Type**                   | **Key Metrics**              | **Expected Outputs**                                      |
| -------------------------------- | ---------------------------- | --------------------------------------------------------- |
| **Deep Learning (MLP)**          | ROC-AUC, F1                  | Classification Report, Confusion Matrix                   |
| **Reinforcement Learning (CQL)** | Estimated Policy Value (EPV) | Approval Rate, Approved-Paid vs Approved-Defaulted Counts |
| **Comparison Report**            | AUC/F1 vs EPV                | Policy Disagreement Cases + Summary Analysis              |

---

## üß† Libraries & References

* **PyTorch** ‚Äî Deep Learning framework
* **scikit-learn** ‚Äî Preprocessing, metrics, data splitting
* **d3rlpy** ‚Äî Offline RL (Conservative Q-Learning, FQE)
* **gymnasium** ‚Äî RL environment dependencies

---

## üìà Results Summary

| **Model**           | **Metric**                 | **Score**        |
| ------------------- | -------------------------- | ---------------- |
| Deep Learning (MLP) | **F1-Score**               | **0.747**        |
| Deep Learning (MLP) | **AUC**                    | **0.928**        |
| Offline RL (CQL)**  | **Estimated Policy Value** | Profit-Optimized |

These results confirm that the **DL model** effectively predicts credit risk, while the **RL agent** learns a more **reward-oriented policy** balancing profit and risk.

---

## üóæ Requirements

```bash
numpy<2.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
joblib>=1.3.0

# Deep Learning (PyTorch)
torch==2.4.1

# Offline RL (CQL) and dependencies
d3rlpy==2.4.0
gymnasium[classic-control]==0.29.1

# Optional: Jupyter support
ipykernel>=6.29.0
```

---

## ‚öñÔ∏è License

This project is intended **solely for educational and internal evaluation** purposes.
Use responsibly and cite appropriately if adapted for research or academic work.
