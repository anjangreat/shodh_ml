{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13546623,
     "sourceType": "datasetVersion",
     "datasetId": 8603326
    }
   ],
   "dockerImageVersionId": 31154,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:04:22.838794Z",
     "iopub.execute_input": "2025-10-29T22:04:22.839071Z",
     "iopub.status.idle": "2025-10-29T22:04:23.173196Z",
     "shell.execute_reply.started": "2025-10-29T22:04:22.839046Z",
     "shell.execute_reply": "2025-10-29T22:04:23.172432Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "execution_count": 1,
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport os\n\n# --- Configuration ---\nprint(\"--- [Notebook Pipeline - Model 7] Part 1: Loading & Initial Setup ---\")\nDATA_FILE_PATH = '/kaggle/input/shodhh/accepted_2007_to_2018Q4.csv'\n\n# --- 1. Load Data ---\ntry:\n    df_nb = pd.read_csv(DATA_FILE_PATH, low_memory=False)\n    # Drop rows where loan_status is missing (as done in the notebook)\n    df_nb = df_nb.dropna(subset=[\"loan_status\"])\n    print(f\"✅ Full data loaded. Shape: {df_nb.shape}\")\nexcept Exception as e:\n    print(f\"❌ Error loading data: {e}\")\n    df_nb = pd.DataFrame()\n\nif not df_nb.empty:\n    # --- 2. Sample Data (as done in the notebook) ---\n    print(\"\\nSampling 100,000 rows...\")\n    sampled_df_nb = df_nb.sample(n=100000, random_state=42)\n    print(f\"Sampled data shape: {sampled_df_nb.shape}\")\n\n    # --- 3. Define Target Variable (Notebook's Definition) ---\n    print(\"Defining target variable 'loan_condition_int' (notebook definition)...\")\n    bad_loan_statuses_nb = [\n        \"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\",\n        \"In Grace Period\", \"Late (16-30 days)\", \"Late (31-120 days)\"\n    ]\n    sampled_df_nb['loan_condition_int'] = sampled_df_nb['loan_status'].apply(\n        lambda status: 1 if status in bad_loan_statuses_nb else 0\n    ).astype(int)\n    # Also create the string version for reference if needed later\n    sampled_df_nb['loan_condition'] = np.where(sampled_df_nb['loan_condition_int'] == 0, 'Good Loan', 'Bad Loan')\n    print(\"Target variable defined.\")\n    print(\"Target distribution in sample:\")\n    print(sampled_df_nb['loan_condition_int'].value_counts(normalize=True))\n\n\n    # --- 4. Map emp_length (Notebook's Mapping) ---\n    print(\"\\nMapping 'emp_length' to 'emp_length_int'...\")\n    emp_length_mapping_nb = {\n        '10+ years': 10, '9 years': 9, '8 years': 8, '7 years': 7, '6 years': 6,\n        '5 years': 5, '4 years': 4, '3 years': 3, '2 years': 2, '1 year': 1,\n        '< 1 year': 0.5, 'n/a': 0\n    }\n    sampled_df_nb['emp_length_int'] = sampled_df_nb['emp_length'].map(emp_length_mapping_nb)\n\n    # --- 5. Map Region (Notebook's Mapping) ---\n    print(\"Mapping 'addr_state' to 'region'...\")\n    state_to_region_nb = {\n        'CA': 'West', 'OR': 'West', 'UT': 'West', 'WA': 'West', 'CO': 'West', 'NV': 'West',\n        'AK': 'West', 'MT': 'West', 'HI': 'West', 'WY': 'West', 'ID': 'West', 'AZ': 'SouthWest',\n        'TX': 'SouthWest', 'NM': 'SouthWest', 'OK': 'SouthWest', 'GA': 'SouthEast', 'NC': 'SouthEast',\n        'VA': 'SouthEast', 'FL': 'SouthEast', 'KY': 'SouthEast', 'SC': 'SouthEast', 'LA': 'SouthEast',\n        'AL': 'SouthEast', 'WV': 'SouthEast', 'DC': 'SouthEast', 'AR': 'SouthEast', 'DE': 'SouthEast',\n        'MS': 'SouthEast', 'TN': 'SouthEast', 'IL': 'MidWest', 'MO': 'MidWest', 'MN': 'MidWest',\n        'OH': 'MidWest', 'WI': 'MidWest', 'KS': 'MidWest', 'MI': 'MidWest', 'SD': 'MidWest',\n        'IA': 'MidWest', 'NE': 'MidWest', 'IN': 'MidWest', 'ND': 'MidWest', 'CT': 'NorthEast',\n        'NY': 'NorthEast', 'PA': 'NorthEast', 'NJ': 'NorthEast', 'RI': 'NorthEast', 'MA': 'NorthEast',\n        'MD': 'NorthEast', 'VT': 'NorthEast', 'NH': 'NorthEast', 'ME': 'NorthEast'\n    }\n    sampled_df_nb['region'] = sampled_df_nb['addr_state'].map(state_to_region_nb)\n\n    # Store for next step\n    model_7_step1_df = sampled_df_nb\n    print(\"\\n✅ Initial setup complete.\")\n\nelse:\n    print(\"❌ Cannot proceed, data loading failed.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:04:23.174806Z",
     "iopub.execute_input": "2025-10-29T22:04:23.175233Z",
     "iopub.status.idle": "2025-10-29T22:05:38.820612Z",
     "shell.execute_reply.started": "2025-10-29T22:04:23.175213Z",
     "shell.execute_reply": "2025-10-29T22:05:38.819942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 1: Loading & Initial Setup ---\n✅ Full data loaded. Shape: (2260668, 151)\n\nSampling 100,000 rows...\nSampled data shape: (100000, 151)\nDefining target variable 'loan_condition_int' (notebook definition)...\nTarget variable defined.\nTarget distribution in sample:\nloan_condition_int\n0    0.86628\n1    0.13372\nName: proportion, dtype: float64\n\nMapping 'emp_length' to 'emp_length_int'...\nMapping 'addr_state' to 'region'...\n\n✅ Initial setup complete.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\n# Assuming 'model_7_step1_df' is the sampled DataFrame from the previous step\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 2: Data Cleaning (Exclusions) ---\")\n\nif 'model_7_step1_df' in locals() or 'model_7_step1_df' in globals():\n    df_cleaning_nb = model_7_step1_df.copy()\n    original_shape = df_cleaning_nb.shape\n    print(f\"Shape before cleaning: {original_shape}\")\n\n    # --- 1. Remove 'Current' and 'Issued' loan_status ---\n    print(\"\\nRemoving 'Current' and 'Issued' loan statuses...\")\n    initial_rows = len(df_cleaning_nb)\n    df_cleaning_nb = df_cleaning_nb[~df_cleaning_nb['loan_status'].isin(['Current', 'Issued'])]\n    rows_removed = initial_rows - len(df_cleaning_nb)\n    print(f\"Removed {rows_removed} rows. New shape: {df_cleaning_nb.shape}\")\n\n    # --- 2. Drop columns with > 80% missing values ---\n    print(\"\\nDropping columns with > 80% missing values...\")\n    initial_cols = df_cleaning_nb.shape[1]\n    # Keep columns with at least 20% non-missing data\n    df_cleaning_nb = df_cleaning_nb.dropna(axis=1, thresh=int(0.20 * len(df_cleaning_nb)))\n    cols_dropped = initial_cols - df_cleaning_nb.shape[1]\n    print(f\"Dropped {cols_dropped} columns. New shape: {df_cleaning_nb.shape}\")\n\n    # --- 3. Drop direct indicator columns (as defined in notebook) ---\n    print(\"\\nDropping direct indicator columns...\")\n    direct_indicators_nb = [\n        'collection_recovery_fee', 'last_pymnt_amnt', 'out_prncp', 'out_prncp_inv',\n        'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int',\n        'total_rec_late_fee', 'total_rec_prncp', 'next_pymnt_d' # Added next_pymnt_d based on notebook context\n    ]\n    # Ensure columns exist before dropping\n    direct_indicators_to_drop = [col for col in direct_indicators_nb if col in df_cleaning_nb.columns]\n    df_cleaning_nb.drop(columns=direct_indicators_to_drop, inplace=True, errors='ignore')\n    print(f\"Dropped {len(direct_indicators_to_drop)} indicator columns. New shape: {df_cleaning_nb.shape}\")\n\n\n    # --- 4. Drop repetitive/useless object columns (as defined in notebook) ---\n    print(\"\\nDropping repetitive/useless object columns...\")\n    misc_cols_to_drop_nb = [\n        'emp_length', # Keeping emp_length_int\n        'id', 'emp_title', 'url', 'title', 'zip_code',\n        # Also drop loan_status and loan_condition as loan_condition_int is the target\n        'loan_status', 'loan_condition',\n        # Drop addr_state as region was created\n        'addr_state'\n    ]\n     # Ensure columns exist before dropping\n    misc_cols_to_drop = [col for col in misc_cols_to_drop_nb if col in df_cleaning_nb.columns]\n    df_cleaning_nb.drop(columns=misc_cols_to_drop, inplace=True, errors='ignore')\n    print(f\"Dropped {len(misc_cols_to_drop)} misc columns. New shape: {df_cleaning_nb.shape}\")\n\n    # Store for next step\n    model_7_step2_df = df_cleaning_nb\n    print(\"\\n✅ Exclusion steps complete.\")\n\nelse:\n    print(\"❌ Error: 'model_7_step1_df' not found. Please re-run Part 1.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:05:38.821378Z",
     "iopub.execute_input": "2025-10-29T22:05:38.821628Z",
     "iopub.status.idle": "2025-10-29T22:05:39.260161Z",
     "shell.execute_reply.started": "2025-10-29T22:05:38.821608Z",
     "shell.execute_reply": "2025-10-29T22:05:39.259394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 2: Data Cleaning (Exclusions) ---\nShape before cleaning: (100000, 155)\n\nRemoving 'Current' and 'Issued' loan statuses...\nRemoved 38822 rows. New shape: (61178, 155)\n\nDropping columns with > 80% missing values...\nDropped 40 columns. New shape: (61178, 115)\n\nDropping direct indicator columns...\nDropped 10 indicator columns. New shape: (61178, 105)\n\nDropping repetitive/useless object columns...\nDropped 9 misc columns. New shape: (61178, 96)\n\n✅ Exclusion steps complete.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\n# Assuming 'model_7_step2_df' is the DataFrame after the exclusion steps\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 3: Missing Value Imputation ---\")\n\nif 'model_7_step2_df' in locals() or 'model_7_step2_df' in globals():\n    fillna_df_nb = model_7_step2_df.copy()\n    print(f\"Shape before imputation: {fillna_df_nb.shape}\")\n    print(f\"Total missing values before: {fillna_df_nb.isnull().sum().sum()}\")\n\n    # --- Impute Object Columns (Mode by Region) ---\n    print(\"\\nImputing object columns by region mode...\")\n    object_cols_to_impute = [\"last_pymnt_d\", \"last_credit_pull_d\"]\n    for column in object_cols_to_impute:\n        if column in fillna_df_nb.columns:\n            # Calculate mode for each region\n            mode_map = fillna_df_nb.groupby(\"region\")[column].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n            # Fill NaNs using the map\n            fillna_df_nb[column] = fillna_df_nb.apply(lambda row: mode_map[row['region']] if pd.isnull(row[column]) else row[column], axis=1)\n            # Fallback for any regions that might have had only NaNs (fill with overall mode)\n            overall_mode = fillna_df_nb[column].mode()[0] if not fillna_df_nb[column].mode().empty else 'Unknown'\n            fillna_df_nb[column].fillna(overall_mode, inplace=True)\n\n\n    # --- Impute Numerical Columns (Median by Region) ---\n    print(\"Imputing specific numerical columns by region median...\")\n    median_cols_to_impute = [\"pub_rec\", \"total_acc\", \"emp_length_int\"]\n    for column in median_cols_to_impute:\n        if column in fillna_df_nb.columns:\n            fillna_df_nb[column] = fillna_df_nb.groupby(\"region\")[column].transform(lambda x: x.fillna(x.median()))\n            # Fallback for any remaining NaNs (e.g., if a whole region was NaN)\n            fillna_df_nb[column].fillna(fillna_df_nb[column].median(), inplace=True)\n\n\n    # --- Impute Numerical Columns (Mean by Region) ---\n    print(\"Imputing specific numerical columns by region mean...\")\n    mean_cols_to_impute = [\"annual_inc\", \"delinq_2yrs\"]\n    for column in mean_cols_to_impute:\n         if column in fillna_df_nb.columns:\n            fillna_df_nb[column] = fillna_df_nb.groupby(\"region\")[column].transform(lambda x: x.fillna(x.mean()))\n            # Fallback for any remaining NaNs\n            fillna_df_nb[column].fillna(fillna_df_nb[column].mean(), inplace=True)\n\n    # --- Fill Remaining NaNs with Zero (as per notebook) ---\n    print(\"Filling all remaining NaNs with 0...\")\n    initial_nan_count = fillna_df_nb.isnull().sum().sum()\n    fillna_df_nb.fillna(0, inplace=True)\n    final_nan_count = fillna_df_nb.isnull().sum().sum()\n    print(f\"Filled {initial_nan_count - final_nan_count} remaining NaN values.\")\n\n    # Store for next step\n    model_7_step3_df = fillna_df_nb\n    print(f\"\\n✅ Imputation complete. Final shape: {model_7_step3_df.shape}\")\n    print(f\"Total missing values after: {model_7_step3_df.isnull().sum().sum()}\")\n\nelse:\n    print(\"❌ Error: 'model_7_step2_df' not found. Please re-run Part 2.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:05:39.261605Z",
     "iopub.execute_input": "2025-10-29T22:05:39.261817Z",
     "iopub.status.idle": "2025-10-29T22:05:41.045304Z",
     "shell.execute_reply.started": "2025-10-29T22:05:39.261800Z",
     "shell.execute_reply": "2025-10-29T22:05:41.044518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 3: Missing Value Imputation ---\nShape before imputation: (61178, 96)\nTotal missing values before: 784318\n\nImputing object columns by region mode...\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_172/2802200912.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(overall_mode, inplace=True)\n/tmp/ipykernel_172/2802200912.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(overall_mode, inplace=True)\n/tmp/ipykernel_172/2802200912.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(fillna_df_nb[column].median(), inplace=True)\n/tmp/ipykernel_172/2802200912.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(fillna_df_nb[column].median(), inplace=True)\n/tmp/ipykernel_172/2802200912.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(fillna_df_nb[column].median(), inplace=True)\n/tmp/ipykernel_172/2802200912.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(fillna_df_nb[column].mean(), inplace=True)\n/tmp/ipykernel_172/2802200912.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  fillna_df_nb[column].fillna(fillna_df_nb[column].mean(), inplace=True)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Imputing specific numerical columns by region median...\nImputing specific numerical columns by region mean...\nFilling all remaining NaNs with 0...\nFilled 780549 remaining NaN values.\n\n✅ Imputation complete. Final shape: (61178, 96)\nTotal missing values after: 0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\n# Assuming 'model_7_step3_df' is the DataFrame after imputation\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 4: Removing Outliers ---\")\n\nif 'model_7_step3_df' in locals() or 'model_7_step3_df' in globals():\n    RemoveOutlier_df_nb = model_7_step3_df.copy()\n    print(f\"Shape before removing outliers: {RemoveOutlier_df_nb.shape}\")\n\n    # Apply custom thresholds as used in the notebook\n    initial_rows = len(RemoveOutlier_df_nb)\n\n    if 'annual_inc' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['annual_inc'] <= 250000]\n    if 'dti' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['dti'] <= 50]\n    if 'open_acc' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['open_acc'] <= 40]\n    if 'total_acc' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['total_acc'] <= 80]\n    if 'revol_util' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['revol_util'] <= 120]\n    if 'revol_bal' in RemoveOutlier_df_nb.columns:\n        RemoveOutlier_df_nb = RemoveOutlier_df_nb[RemoveOutlier_df_nb['revol_bal'] <= 250000]\n\n    # Reset index after filtering\n    RemoveOutlier_df_nb.reset_index(drop=True, inplace=True)\n\n    rows_removed = initial_rows - len(RemoveOutlier_df_nb)\n    print(f\"Removed {rows_removed} rows due to outlier thresholds.\")\n    print(f\"Shape after removing outliers: {RemoveOutlier_df_nb.shape}\")\n\n    # Store for next step\n    model_7_step4_df = RemoveOutlier_df_nb\n    print(\"\\n✅ Outlier removal complete.\")\n\nelse:\n    print(\"❌ Error: 'model_7_step3_df' not found. Please re-run Part 3.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:05:41.046336Z",
     "iopub.execute_input": "2025-10-29T22:05:41.046686Z",
     "iopub.status.idle": "2025-10-29T22:05:41.193901Z",
     "shell.execute_reply.started": "2025-10-29T22:05:41.046663Z",
     "shell.execute_reply": "2025-10-29T22:05:41.193049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 4: Removing Outliers ---\nShape before removing outliers: (61178, 96)\nRemoved 925 rows due to outlier thresholds.\nShape after removing outliers: (60253, 96)\n\n✅ Outlier removal complete.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Ensure category_encoders is installed\ntry:\n    from category_encoders import TargetEncoder\nexcept ImportError:\n    print(\"Warning: category_encoders not found. Attempting install...\")\n    try:\n        import sys\n        !{sys.executable} -m pip install category_encoders --quiet\n        from category_encoders import TargetEncoder\n        print(\"Installation successful.\")\n    except Exception as e:\n        print(f\"Error installing category_encoders: {e}\")\n        TargetEncoder = None\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming 'model_7_step4_df' is the DataFrame after outlier removal\n# Assuming 'loan_condition_int' is the target column name\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 5: Feature Engineering ---\")\n\nif 'model_7_step4_df' in locals() or 'model_7_step4_df' in globals():\n    FE_df_nb = model_7_step4_df.copy()\n    target_col_nb = 'loan_condition_int'\n\n    # --- 1. Identify Feature Types ---\n    original_cols_fe = FE_df_nb.columns.tolist()\n    cat_cols_fe = FE_df_nb.select_dtypes(include=['object']).columns.tolist()\n    # Exclude the target variable from numerical columns\n    num_cols_fe = FE_df_nb.select_dtypes(exclude=['object']).columns.drop(target_col_nb, errors='ignore').tolist()\n\n    # Separate categorical into binary and multi-category\n    dual_cat_cols_fe = [col for col in cat_cols_fe if FE_df_nb[col].nunique() <= 2]\n    multi_cat_cols_fe = [col for col in cat_cols_fe if FE_df_nb[col].nunique() > 2]\n\n    print(f\"Numerical columns found: {len(num_cols_fe)}\")\n    print(f\"Binary categorical columns: {dual_cat_cols_fe}\")\n    print(f\"Multi-categorical columns: {multi_cat_cols_fe}\")\n\n    # --- 2. Binary Encoding (get_dummies) ---\n    print(\"\\nApplying Binary Encoding (get_dummies)...\")\n    FE_df_nb = pd.get_dummies(FE_df_nb, columns=dual_cat_cols_fe, drop_first=True)\n    # Get names of newly created binary columns (needed later if scaling)\n    new_binary_cols = [col for col in FE_df_nb.columns if col not in original_cols_fe and col != target_col_nb]\n    print(f\"Created {len(new_binary_cols)} new binary columns.\")\n\n    # --- 3. Train/Test Split (Stratified) ---\n    print(\"\\nSplitting data into training (80%) and test (20%) sets...\")\n    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n    # Perform the split\n    for train_idx, test_idx in stratified_split.split(FE_df_nb, FE_df_nb[target_col_nb]):\n        train_df_nb = FE_df_nb.loc[train_idx]\n        test_df_nb = FE_df_nb.loc[test_idx]\n\n    # Separate features and target\n    train_y_nb = train_df_nb[[target_col_nb]]\n    test_y_nb = test_df_nb[[target_col_nb]]\n    train_X_nb = train_df_nb.drop(target_col_nb, axis=1)\n    test_X_nb = test_df_nb.drop(target_col_nb, axis=1)\n\n    print(f\"Training set shape: X={train_X_nb.shape}, y={train_y_nb.shape}\")\n    print(f\"Test set shape:     X={test_X_nb.shape}, y={test_y_nb.shape}\")\n\n    # --- 4. Target Encoding ---\n    if TargetEncoder is not None and multi_cat_cols_fe:\n        print(\"\\nApplying Target Encoding...\")\n        # Ensure only existing multi-cat columns are processed\n        multi_cat_cols_to_encode = [col for col in multi_cat_cols_fe if col in train_X_nb.columns]\n        target_encoder_nb = TargetEncoder(cols=multi_cat_cols_to_encode, smoothing=0.2) # Notebook used smoothing=0.2\n\n        # Fit ONLY on training data\n        target_encoder_nb.fit(train_X_nb, train_y_nb.values.ravel()) # .values.ravel() converts to 1D array\n\n        # Transform both train and test data\n        train_X_encoded = target_encoder_nb.transform(train_X_nb)\n        test_X_encoded = target_encoder_nb.transform(test_X_nb)\n        print(\"Target Encoding applied.\")\n        # Store list of newly numerical columns from target encoding\n        target_encoded_numeric_cols = multi_cat_cols_to_encode\n    elif TargetEncoder is None:\n        print(\"❌ Skipping Target Encoding as category_encoders is not available.\")\n        train_X_encoded = train_X_nb.copy()\n        test_X_encoded = test_X_nb.copy()\n        target_encoded_numeric_cols = []\n    else:\n        print(\"No multi-category columns found for Target Encoding.\")\n        train_X_encoded = train_X_nb.copy()\n        test_X_encoded = test_X_nb.copy()\n        target_encoded_numeric_cols = []\n\n\n    # --- 5. Normalization (StandardScaler) ---\n    print(\"\\nApplying Normalization (StandardScaler)...\")\n    scaler_nb = StandardScaler()\n\n    # Identify all numerical columns for scaling (original + target encoded + new binary)\n    # Ensure binary columns from get_dummies are treated as numerical for scaling\n    cols_to_scale = num_cols_fe + target_encoded_numeric_cols + new_binary_cols\n    # Filter out any columns that might have been dropped or don't exist\n    cols_to_scale = [col for col in cols_to_scale if col in train_X_encoded.columns]\n\n\n    # Fit ONLY on training data\n    print(f\"Fitting scaler on {len(cols_to_scale)} numerical features...\")\n    scaler_nb.fit(train_X_encoded[cols_to_scale])\n\n    # Transform both train and test data (in place)\n    train_X_scaled = train_X_encoded.copy()\n    test_X_scaled = test_X_encoded.copy()\n\n    train_X_scaled[cols_to_scale] = scaler_nb.transform(train_X_encoded[cols_to_scale])\n    test_X_scaled[cols_to_scale] = scaler_nb.transform(test_X_encoded[cols_to_scale])\n    print(\"Normalization applied.\")\n\n    # Store final datasets for next step\n    model_7_step5_train_X = train_X_scaled\n    model_7_step5_train_y = train_y_nb\n    model_7_step5_test_X = test_X_scaled\n    model_7_step5_test_y = test_y_nb\n\n    print(f\"\\n✅ Feature Engineering complete.\")\n    print(f\"Final training X shape: {model_7_step5_train_X.shape}\")\n    print(f\"Final test X shape: {model_7_step5_test_X.shape}\")\n\nelse:\n    print(\"❌ Error: 'model_7_step4_df' not found. Please re-run Part 4.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:05:41.194774Z",
     "iopub.execute_input": "2025-10-29T22:05:41.195039Z",
     "iopub.status.idle": "2025-10-29T22:05:43.398621Z",
     "shell.execute_reply.started": "2025-10-29T22:05:41.195018Z",
     "shell.execute_reply": "2025-10-29T22:05:43.397715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 5: Feature Engineering ---\nNumerical columns found: 78\nBinary categorical columns: ['term', 'pymnt_plan', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']\nMulti-categorical columns: ['grade', 'sub_grade', 'home_ownership', 'verification_status', 'issue_d', 'purpose', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d', 'region']\n\nApplying Binary Encoding (get_dummies)...\nCreated 7 new binary columns.\n\nSplitting data into training (80%) and test (20%) sets...\nTraining set shape: X=(48202, 95), y=(48202, 1)\nTest set shape:     X=(12051, 95), y=(12051, 1)\n\nApplying Target Encoding...\nTarget Encoding applied.\n\nApplying Normalization (StandardScaler)...\nFitting scaler on 95 numerical features...\nNormalization applied.\n\n✅ Feature Engineering complete.\nFinal training X shape: (48202, 95)\nFinal test X shape: (12051, 95)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Install compatible scikit-learn\n!pip install scikit-learn==1.5.2 --force-reinstall --quiet\n# Install imbalanced-learn\n!pip install imbalanced-learn --quiet\n\nprint(\"✅ Installations attempted again. Please check output.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:05:43.399540Z",
     "iopub.execute_input": "2025-10-29T22:05:43.399924Z",
     "iopub.status.idle": "2025-10-29T22:06:03.152043Z",
     "shell.execute_reply.started": "2025-10-29T22:05:43.399902Z",
     "shell.execute_reply": "2025-10-29T22:06:03.150979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m✅ Installations attempted again. Please check output.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n# Ensure imblearn is installed\ntry:\n    from imblearn.under_sampling import RandomUnderSampler\n    print(\"✅ imblearn imported successfully.\")\nexcept ImportError:\n    print(\"❌ Error: imbalanced-learn not found or import failed.\")\n    print(\"Please ensure imbalanced-learn is installed (e.g., !pip install imbalanced-learn --quiet).\")\n    RandomUnderSampler = None\n\n# --- Setup ---\n# Assuming model_7_step5_train_X, model_7_step5_train_y,\n# model_7_step5_test_X, model_7_step5_test_y are from Step 5\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 6: Applying Random Undersampling ---\")\n\n# Check if inputs exist and RandomUnderSampler is available\nif RandomUnderSampler is None:\n    print(\"❌ Cannot proceed without RandomUnderSampler.\")\nelif 'model_7_step5_train_X' not in locals() or 'model_7_step5_train_y' not in locals():\n    print(\"❌ Error: Input training data not found. Please ensure Step 5 completed successfully.\")\nelse:\n    # --- 1. Initialize Undersampler ---\n    rus_nb = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n\n    # --- 2. Apply Undersampling ONLY to Training Data ---\n    print(f\"Original training data shape: X={model_7_step5_train_X.shape}, y={model_7_step5_train_y.shape}\")\n    print(\"Original training target distribution:\")\n    print(model_7_step5_train_y['loan_condition_int'].value_counts()) # Access column in DataFrame\n\n    try:\n        # Pass DataFrame/Series directly\n        X_train_undersampled_nb, y_train_undersampled_nb = rus_nb.fit_resample(\n            model_7_step5_train_X, model_7_step5_train_y['loan_condition_int'] # Pass Series\n        )\n\n        print(f\"\\nUndersampled training data shape: X={X_train_undersampled_nb.shape}, y={y_train_undersampled_nb.shape}\")\n        print(\"Undersampled training target distribution:\")\n        print(y_train_undersampled_nb.value_counts()) # Now it's a Series\n\n        # --- Store Final Datasets for Modeling ---\n        # NOTE: The notebook used these undersampled sets directly for feature selection next\n        model_7_step6_train_X = X_train_undersampled_nb\n        model_7_step6_train_y = y_train_undersampled_nb # This is now a Series\n\n        # Keep the original test set (encoded and scaled but not undersampled)\n        model_7_step6_test_X = model_7_step5_test_X\n        model_7_step6_test_y = model_7_step5_test_y # This is still a DataFrame\n\n        print(\"\\n✅ Undersampling complete. Datasets ready for feature selection.\")\n\n    except Exception as e:\n        print(f\"❌ An error occurred during fit_resample: {e}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:06:03.153543Z",
     "iopub.execute_input": "2025-10-29T22:06:03.153940Z",
     "iopub.status.idle": "2025-10-29T22:06:03.555211Z",
     "shell.execute_reply.started": "2025-10-29T22:06:03.153889Z",
     "shell.execute_reply": "2025-10-29T22:06:03.554303Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x781950f4e5c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n    self._make_controller_from_path(filepath)\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n    lib_controller = controller_class(\n                     ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: dlopen() error\nException ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x781950f4e5c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n    self._make_controller_from_path(filepath)\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n    lib_controller = controller_class(\n                     ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: dlopen() error\nException ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x781950f4e5c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n    self._make_controller_from_path(filepath)\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n    lib_controller = controller_class(\n                     ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n    self._handle = _dlopen(self._name, mode)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\nOSError: dlopen() error\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "✅ imblearn imported successfully.\n--- [Notebook Pipeline - Model 7] Part 6: Applying Random Undersampling ---\nOriginal training data shape: X=(48202, 95), y=(48202, 1)\nOriginal training target distribution:\nloan_condition_int\n0    37664\n1    10538\nName: count, dtype: int64\n\nUndersampled training data shape: X=(21076, 95), y=(21076,)\nUndersampled training target distribution:\nloan_condition_int\n0    10538\n1    10538\nName: count, dtype: int64\n\n✅ Undersampling complete. Datasets ready for feature selection.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import VarianceThreshold\n\n# --- Setup ---\n# Assuming model_7_step6_train_X (undersampled, scaled, encoded training features) exists\n# Assuming model_7_step6_test_X (original scaled, encoded test features) exists\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 7: Feature Selection ---\")\n\nif 'model_7_step6_train_X' not in locals() or 'model_7_step6_test_X' not in locals():\n    print(\"❌ Error: Input data (model_7_step6_train_X or model_7_step6_test_X) not found.\")\n    print(\"Please ensure Step 6 (Undersampling) completed successfully.\")\nelse:\n    X_train_fs = model_7_step6_train_X.copy()\n    X_test_fs = model_7_step6_test_X.copy() # Apply selection to test set too\n    # The notebook implicitly uses the undersampled y_train for wrapper, but we only need X for VarianceThreshold\n    # y_train_fs = model_7_step6_train_y # Undersampled training target\n\n    print(f\"Shape before VarianceThreshold: {X_train_fs.shape}\")\n\n    # --- 1. Apply VarianceThreshold ---\n    # The notebook used threshold=1 on the *scaled* data\n    selector = VarianceThreshold(threshold=1)\n    selector.fit(X_train_fs)\n\n    # Get the names of the features kept by the threshold\n    filtered_feature_names = X_train_fs.columns[selector.get_support()]\n    X_train_variance_filtered = X_train_fs[filtered_feature_names]\n\n    cols_removed = X_train_fs.shape[1] - X_train_variance_filtered.shape[1]\n    print(f\"Applied VarianceThreshold(1). Removed {cols_removed} features.\")\n    print(f\"Shape after VarianceThreshold: {X_train_variance_filtered.shape}\")\n\n    # --- 2. Select Final Features (Based on Notebook's Wrapper Result) ---\n    # The notebook ran a time-consuming wrapper (SFS) and identified these 9 features.\n    # We will directly select these for replication purposes.\n    vars_final_nb = [\n        'delinq_2yrs',\n        'last_fico_range_high',\n        'last_fico_range_low',\n        'acc_now_delinq',\n        'open_acc_6m',\n        'total_bal_il',\n        'il_util',\n        'open_rv_12m',\n        'all_util'\n     ]\n    print(f\"\\nSelecting the final {len(vars_final_nb)} features identified by the notebook's wrapper method...\")\n\n    # Ensure these final columns actually exist after variance thresholding\n    final_cols_exist = [col for col in vars_final_nb if col in X_train_variance_filtered.columns]\n\n    if len(final_cols_exist) != len(vars_final_nb):\n        print(f\"⚠️ Warning: Not all expected final features ({vars_final_nb}) were present after VarianceThreshold.\")\n        print(f\"Features missing: {list(set(vars_final_nb) - set(final_cols_exist))}\")\n        print(f\"Proceeding with the {len(final_cols_exist)} available features: {final_cols_exist}\")\n        final_selected_cols = final_cols_exist\n    else:\n        final_selected_cols = vars_final_nb\n        print(\"All expected final features found.\")\n\n\n    # Apply the final selection to both train and test sets\n    X_train_final_selected = X_train_variance_filtered[final_selected_cols]\n    # Also apply to the original test set (after variance filter applied to it)\n    X_test_variance_filtered = X_test_fs[filtered_feature_names] # Apply VT filter result\n    X_test_final_selected = X_test_variance_filtered[final_selected_cols] # Select final cols\n\n\n    # Store final datasets for modeling\n    model_7_step7_train_X = X_train_final_selected\n    model_7_step7_train_y = model_7_step6_train_y # Use the undersampled y from step 6\n    model_7_step7_test_X = X_test_final_selected\n    model_7_step7_test_y = model_7_step6_test_y # Use the original test y from step 6\n\n    print(f\"\\n✅ Feature Selection complete.\")\n    print(f\"Final training X shape: {model_7_step7_train_X.shape}\")\n    print(f\"Final test X shape: {model_7_step7_test_X.shape}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:06:03.556153Z",
     "iopub.execute_input": "2025-10-29T22:06:03.556552Z",
     "iopub.status.idle": "2025-10-29T22:06:03.614358Z",
     "shell.execute_reply.started": "2025-10-29T22:06:03.556522Z",
     "shell.execute_reply": "2025-10-29T22:06:03.613474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 7: Feature Selection ---\nShape before VarianceThreshold: (21076, 95)\nApplied VarianceThreshold(1). Removed 37 features.\nShape after VarianceThreshold: (21076, 58)\n\nSelecting the final 9 features identified by the notebook's wrapper method...\nAll expected final features found.\n\n✅ Feature Selection complete.\nFinal training X shape: (21076, 9)\nFinal test X shape: (12051, 9)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "!pip install numpy==1.26.4 --force-reinstall --quiet\nprint(\"✅ NumPy downgrade attempted. Please check output.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:06:03.616309Z",
     "iopub.execute_input": "2025-10-29T22:06:03.616616Z",
     "iopub.status.idle": "2025-10-29T22:06:10.383365Z",
     "shell.execute_reply.started": "2025-10-29T22:06:03.616596Z",
     "shell.execute_reply": "2025-10-29T22:06:10.382562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.0/61.0 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m75.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m✅ NumPy downgrade attempted. Please check output.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n# import matplotlib.pyplot as plt # Removed\n# import seaborn as sns # Removed\nimport scipy.stats as sps\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             recall_score, f1_score, roc_auc_score)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb # Ensure xgboost is installed if needed: !pip install xgboost --quiet\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Suppress convergence warnings etc.\n\n# --- Setup ---\n# Assuming model_7_step7_train_X, model_7_step7_train_y exist from Step 7\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 8 (No Plotting): Model Building & CV ---\")\n\n# Check if input data exists\nif 'model_7_step7_train_X' not in locals() or 'model_7_step7_train_y' not in locals():\n    print(\"❌ Error: Input training data not found. Please ensure Step 7 completed successfully.\")\nelse:\n    X = model_7_step7_train_X\n    # Ensure y is a Series or 1D array\n    if isinstance(model_7_step7_train_y, pd.DataFrame):\n        y = model_7_step7_train_y.iloc[:, 0]\n    else:\n        y = model_7_step7_train_y\n\n    # --- 1. Define Helper Functions (Plotting Removed) ---\n\n    # Removed plot_confusion_matrix function\n\n    def calculate_scores(model, X_trn, y_trn, X_tst, y_tst):\n        # ... (rest of the function is the same as before) ...\n        model.fit(X_trn, y_trn)\n        y_pred = model.predict(X_tst)\n        accuracy = accuracy_score(y_tst, y_pred)\n        conf_matrix = confusion_matrix(y_tst, y_pred)\n        precision = precision_score(y_tst, y_pred, zero_division=0)\n        recall = recall_score(y_tst, y_pred, zero_division=0)\n        f1 = f1_score(y_tst, y_pred, zero_division=0)\n        try:\n            y_pred_proba = model.predict_proba(X_tst)[:, 1]\n            auc = roc_auc_score(y_tst, y_pred_proba)\n            mask = np.array(y_tst).astype(bool)\n            churn = y_pred_proba[mask]\n            not_churn = y_pred_proba[~mask]\n            ks = sps.ks_2samp(churn, not_churn)[0] if len(churn) > 0 and len(not_churn) > 0 else 0.0\n        except AttributeError:\n            auc = 0.0\n            ks = 0.0\n            print(f\"Warning: {type(model).__name__} does not have predict_proba. AUC/KS set to 0.\")\n        except Exception as e_proba: # Catch other potential errors during predict_proba\n             auc = 0.0\n             ks = 0.0\n             print(f\"Warning: Error during predict_proba for {type(model).__name__}: {e_proba}. AUC/KS set to 0.\")\n\n\n        return accuracy, auc, ks, conf_matrix, precision, recall, f1\n\n    def calculate_cv_scores(model, X, y, cv=5):\n        # ... (setup is the same) ...\n        y_array = y.values if isinstance(y, pd.Series) else np.array(y)\n        kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n        accuracy_scores, auc_scores, ks_scores = [], [], []\n        conf_matrices, precision_scores, recall_scores, f1_scores = [], [], [], []\n\n        for fold, (train_index, test_index) in enumerate(kf.split(X)):\n             # ... (splitting is the same) ...\n            X_trn, X_tst = X.iloc[train_index], X.iloc[test_index]\n            y_trn, y_tst = y.iloc[train_index] if isinstance(y, pd.Series) else y_array[train_index], \\\n                           y.iloc[test_index] if isinstance(y, pd.Series) else y_array[test_index]\n\n            try:\n                accuracy, auc, ks, conf_matrix, precision, recall, f1 = calculate_scores(model, X_trn, y_trn, X_tst, y_tst)\n                accuracy_scores.append(accuracy)\n                auc_scores.append(auc)\n                ks_scores.append(ks)\n                conf_matrices.append(conf_matrix)\n                precision_scores.append(precision)\n                recall_scores.append(recall)\n                f1_scores.append(f1)\n            except Exception as e_fold:\n                print(f\"Error during CV fold {fold+1} for {type(model).__name__}: {e_fold}\")\n                # Append NaNs or handle error as appropriate\n                accuracy_scores.append(np.nan)\n                auc_scores.append(np.nan)\n                ks_scores.append(np.nan)\n                # conf_matrices.append(np.array([[np.nan, np.nan], [np.nan, np.nan]])) # Placeholder\n                precision_scores.append(np.nan)\n                recall_scores.append(np.nan)\n                f1_scores.append(np.nan)\n\n\n        # Calculate mean confusion matrix, handling potential empty lists or errors\n        if conf_matrices:\n             # Ensure all matrices have the same shape before averaging (e.g., handle folds that failed)\n             valid_matrices = [cm for cm in conf_matrices if isinstance(cm, np.ndarray) and cm.shape == (2, 2)]\n             if valid_matrices:\n                  mean_conf_matrix = np.mean(valid_matrices, axis=0)\n                  print(f\"\\nMean Confusion Matrix for {type(model).__name__} (GoodLoan=0, BadLoan=1):\\n\", mean_conf_matrix)\n             else:\n                  print(f\"\\nCould not calculate Mean Confusion Matrix for {type(model).__name__} due to errors in folds.\")\n        else:\n             print(f\"\\nNo Confusion Matrices generated for {type(model).__name__}.\")\n\n\n        # Calculate mean scores, ignoring NaNs from failed folds\n        final_accuracy = np.nanmean(accuracy_scores) if accuracy_scores else 0.0\n        final_auc = np.nanmean(auc_scores) if auc_scores else 0.0\n        final_ks = np.nanmean(ks_scores) if ks_scores else 0.0\n        final_precision = np.nanmean(precision_scores) if precision_scores else 0.0\n        final_recall = np.nanmean(recall_scores) if recall_scores else 0.0\n        final_f1 = np.nanmean(f1_scores) if f1_scores else 0.0\n\n        return final_accuracy, final_auc, final_ks, final_precision, final_recall, final_f1\n\n\n    def fit_models_summary(models, X, y, cv=5):\n        # ... (rest of the function is the same as before) ...\n        summary = pd.DataFrame(columns=['accuracy', 'auc', 'ks', 'precision', 'recall', 'f1'])\n        for name, model in models.items():\n            print(f\"--- Processing: {name} ---\")\n            start_time = time.time()\n            try:\n                accuracy, auc, ks, precision, recall, f1 = calculate_cv_scores(model, X, y, cv=cv)\n                summary.loc[name] = [accuracy, auc, ks, precision, recall, f1]\n            except Exception as e_model:\n                 print(f\"!!!!! Error processing model {name}: {e_model} !!!!!\")\n                 summary.loc[name] = [np.nan] * 6 # Add row with NaNs\n\n            end_time = time.time()\n            print(f\"{name} processed in {end_time - start_time:.2f} seconds.\")\n            print(\"-\" * (len(name) + 20))\n        return summary.sort_values(by='f1', ascending=False)\n\n\n    # --- 2. Define Models (Same as before) ---\n    mss=60\n    msl=int(mss/2)\n\n    models_nb = {\n         'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n         'Decision Tree': DecisionTreeClassifier(max_depth=10, min_samples_split=mss, min_samples_leaf=msl, random_state=42),\n         'K Nearest Neighbors': KNeighborsClassifier(n_neighbors=20, n_jobs=-1),\n         'Random Forest': RandomForestClassifier(n_estimators=20, max_depth=10, random_state=42, n_jobs=-1),\n         'Gaussian Naive Bayes': GaussianNB(var_smoothing=1),\n         'Light GBM': lgb.LGBMClassifier(n_estimators=50, max_depth=3, random_state=42, n_jobs=-1, verbose=-1),\n         'XGBoost': xgb.XGBClassifier(n_estimators=50, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n         'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n         \"Neural Network\": MLPClassifier(hidden_layer_sizes=(10,10), random_state=42, max_iter=500)\n    }\n\n    # --- 3. Run Cross-Validation ---\n    print(\"\\nStarting 5-fold cross-validation for all models (plotting disabled)...\")\n    start_cv_time = time.time()\n    baseline_summary_nb = fit_models_summary(models_nb, X, y, cv=5)\n    end_cv_time = time.time()\n    print(f\"\\nCross-validation finished in {end_cv_time - start_cv_time:.2f} seconds.\")\n\n    # Display summary table\n    print(\"\\n--- Cross-Validation Summary (Sorted by F1-Score) ---\")\n    print(baseline_summary_nb)\n\n    # Store results\n    model_7_cv_summary = baseline_summary_nb",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:09:44.865113Z",
     "iopub.execute_input": "2025-10-29T22:09:44.865986Z",
     "iopub.status.idle": "2025-10-29T22:10:12.560281Z",
     "shell.execute_reply.started": "2025-10-29T22:09:44.865947Z",
     "shell.execute_reply": "2025-10-29T22:10:12.559558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.3.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_172/3335889291.py\", line 15, in <cell line: 0>\n    import lightgbm as lgb\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/__init__.py\", line 11, in <module>\n    from .basic import Booster, Dataset, Sequence, register_logger\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py\", line 29, in <module>\n    from .compat import (\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/compat.py\", line 191, in <module>\n    import matplotlib  # noqa: F401\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;31mAttributeError\u001B[0m: _ARRAY_API not found"
     ],
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error"
    },
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 8 (No Plotting): Model Building & CV ---\n\nStarting 5-fold cross-validation for all models (plotting disabled)...\n--- Processing: Logistic Regression ---\n\nMean Confusion Matrix for LogisticRegression (GoodLoan=0, BadLoan=1):\n [[1850.2  257.4]\n [ 301.2 1806.4]]\nLogistic Regression processed in 0.46 seconds.\n---------------------------------------\n--- Processing: Decision Tree ---\n\nMean Confusion Matrix for DecisionTreeClassifier (GoodLoan=0, BadLoan=1):\n [[1797.4  310.2]\n [ 257.6 1850. ]]\nDecision Tree processed in 0.44 seconds.\n---------------------------------\n--- Processing: K Nearest Neighbors ---\n\nMean Confusion Matrix for KNeighborsClassifier (GoodLoan=0, BadLoan=1):\n [[1972.8  134.8]\n [ 937.2 1170.4]]\nK Nearest Neighbors processed in 3.97 seconds.\n---------------------------------------\n--- Processing: Random Forest ---\n\nMean Confusion Matrix for RandomForestClassifier (GoodLoan=0, BadLoan=1):\n [[1804.   303.6]\n [ 256.2 1851.4]]\nRandom Forest processed in 0.96 seconds.\n---------------------------------\n--- Processing: Gaussian Naive Bayes ---\n\nMean Confusion Matrix for GaussianNB (GoodLoan=0, BadLoan=1):\n [[1961.8  145.8]\n [ 751.6 1356. ]]\nGaussian Naive Bayes processed in 0.31 seconds.\n----------------------------------------\n--- Processing: Light GBM ---\n\nMean Confusion Matrix for LGBMClassifier (GoodLoan=0, BadLoan=1):\n [[1811.6  296. ]\n [ 251.6 1856. ]]\nLight GBM processed in 0.60 seconds.\n-----------------------------\n--- Processing: XGBoost ---\n\nMean Confusion Matrix for XGBClassifier (GoodLoan=0, BadLoan=1):\n [[1814.   293.6]\n [ 256.4 1851.2]]\nXGBoost processed in 0.64 seconds.\n---------------------------\n--- Processing: Gradient Boosting ---\n\nMean Confusion Matrix for GradientBoostingClassifier (GoodLoan=0, BadLoan=1):\n [[1815.8  291.8]\n [ 254.6 1853. ]]\nGradient Boosting processed in 3.77 seconds.\n-------------------------------------\n--- Processing: Neural Network ---\n\nMean Confusion Matrix for MLPClassifier (GoodLoan=0, BadLoan=1):\n [[1808.   299.6]\n [ 246.2 1861.4]]\nNeural Network processed in 12.40 seconds.\n----------------------------------\n\nCross-validation finished in 23.55 seconds.\n\n--- Cross-Validation Summary (Sorted by F1-Score) ---\n                      accuracy       auc        ks  precision    recall  \\\nNeural Network        0.870516  0.931020  0.742971   0.861349  0.883275   \nGradient Boosting     0.870374  0.931753  0.744043   0.863966  0.879169   \nLight GBM             0.870089  0.931906  0.743190   0.862485  0.880557   \nXGBoost               0.869520  0.930852  0.741793   0.863127  0.878326   \nRandom Forest         0.867195  0.929093  0.738927   0.859111  0.878434   \nDecision Tree         0.865297  0.926116  0.736383   0.856405  0.877745   \nLogistic Regression   0.867479  0.927969  0.742333   0.875258  0.857158   \nGaussian Naive Bayes  0.787103  0.910858  0.686400   0.902862  0.643544   \nK Nearest Neighbors   0.745682  0.781268  0.533647   0.896756  0.555248   \n\n                            f1  \nNeural Network        0.872135  \nGradient Boosting     0.871487  \nLight GBM             0.871413  \nXGBoost               0.870646  \nRandom Forest         0.868652  \nDecision Tree         0.866915  \nLogistic Regression   0.866101  \nGaussian Naive Bayes  0.751137  \nK Nearest Neighbors   0.685794  \n",
     "output_type": "stream"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             recall_score, f1_score, roc_auc_score)\nimport scipy.stats as sps\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --- Setup ---\n# Assuming model_7_step7_train_X, model_7_step7_train_y (undersampled train)\n# Assuming model_7_step7_test_X, model_7_step7_test_y (original test) exist from previous steps\n\nprint(\"--- [Notebook Pipeline - Model 7] Part 9: Final Evaluation on Test Set ---\")\n\n# Check if data exists\nif ('model_7_step7_train_X' not in locals() or 'model_7_step7_train_y' not in locals() or\n    'model_7_step7_test_X' not in locals() or 'model_7_step7_test_y' not in locals()):\n    print(\"❌ Error: Input data not found. Please ensure previous steps ran successfully.\")\nelse:\n    X_trn_final = model_7_step7_train_X\n    # Ensure y is Series/1D array\n    y_trn_final = model_7_step7_train_y if isinstance(model_7_step7_train_y, pd.Series) else pd.Series(model_7_step7_train_y)\n    X_tst_final = model_7_step7_test_X\n    # Ensure y is Series/1D array from DataFrame\n    y_tst_final = model_7_step7_test_y.iloc[:, 0] if isinstance(model_7_step7_test_y, pd.DataFrame) else pd.Series(model_7_step7_test_y)\n\n\n    # --- 1. Define Helper Function for Test Set Scores ---\n    def calculate_test_scores(model, X_trn, y_trn, X_tst, y_tst):\n        # Returns: accuracy, auc, ks, conf_matrix, precision, recall, f1\n        # Uses the same logic as the calculate_scores from CV step\n        model.fit(X_trn, y_trn)\n        y_pred = model.predict(X_tst)\n        accuracy = accuracy_score(y_tst, y_pred)\n        conf_matrix = confusion_matrix(y_tst, y_pred)\n        precision = precision_score(y_tst, y_pred, zero_division=0)\n        recall = recall_score(y_tst, y_pred, zero_division=0)\n        f1 = f1_score(y_tst, y_pred, zero_division=0)\n        try:\n            y_pred_proba = model.predict_proba(X_tst)[:, 1]\n            auc = roc_auc_score(y_tst, y_pred_proba)\n            mask = np.array(y_tst).astype(bool)\n            churn = y_pred_proba[mask]\n            not_churn = y_pred_proba[~mask]\n            ks = sps.ks_2samp(churn, not_churn)[0] if len(churn) > 0 and len(not_churn) > 0 else 0.0\n        except AttributeError:\n             auc, ks = 0.0, 0.0\n        except Exception as e_proba:\n             auc, ks = 0.0, 0.0\n             print(f\"Warning: Prob pred error for {type(model).__name__}: {e_proba}. AUC/KS=0.\")\n\n        return accuracy, auc, ks, conf_matrix, precision, recall, f1\n\n    def fit_first_level_preds(models, X_trn, y_trn, X_tst):\n        # Generates predictions for stacking (similar to notebook function)\n        X2_trn = np.zeros((len(X_trn), 2 * len(models)))\n        X2_tst = np.zeros((len(X_tst), 2 * len(models)))\n\n        for i, (name, model) in enumerate(models.items()):\n            print(f\"  Generating stacking preds for: {name}\")\n            try:\n                model.fit(X_trn, y_trn)\n                y_trn_pred_proba = model.predict_proba(X_trn)\n                y_tst_pred_proba = model.predict_proba(X_tst)\n                X2_trn[:, i*2:(i+1)*2] = y_trn_pred_proba\n                X2_tst[:, i*2:(i+1)*2] = y_tst_pred_proba\n            except Exception as e_stack_fit:\n                 print(f\"  Error fitting/predicting {name} for stacking: {e_stack_fit}\")\n                 # Fill with NaNs or zeros if a model fails\n                 X2_trn[:, i*2:(i+1)*2] = np.nan\n                 X2_tst[:, i*2:(i+1)*2] = np.nan\n\n        return X2_trn, X2_tst\n\n\n    # --- 2. Define Models (Same base models as CV) ---\n    mss=60\n    msl=int(mss/2)\n    models_nb_final = {\n         'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n         'Decision Tree': DecisionTreeClassifier(max_depth=10, min_samples_split=mss, min_samples_leaf=msl, random_state=42),\n         'K Nearest Neighbors': KNeighborsClassifier(n_neighbors=20, n_jobs=-1),\n         'Random Forest': RandomForestClassifier(n_estimators=20, max_depth=10, random_state=42, n_jobs=-1),\n         'Gaussian Naive Bayes': GaussianNB(var_smoothing=1),\n         'Light GBM': lgb.LGBMClassifier(n_estimators=50, max_depth=3, random_state=42, n_jobs=-1, verbose=-1),\n         'XGBoost': xgb.XGBClassifier(n_estimators=50, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss'),\n         'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n         \"Neural Network\": MLPClassifier(hidden_layer_sizes=(10,10), random_state=42, max_iter=500)\n    }\n\n    # --- 3. Evaluate Base Models on Test Set ---\n    print(\"\\n--- Evaluating Base Models on Hold-Out Test Set ---\")\n    holdout_summary_nb = pd.DataFrame(columns=['accuracy', 'auc', 'ks', 'precision', 'recall', 'f1'])\n    all_conf_matrices = {} # Store confusion matrices\n    start_base_time = time.time()\n\n    for name, model in models_nb_final.items():\n        print(f\"Processing: {name}\")\n        try:\n            accuracy, auc, ks, conf_matrix, precision, recall, f1 = calculate_test_scores(model, X_trn_final, y_trn_final, X_tst_final, y_tst_final)\n            holdout_summary_nb.loc[name] = [accuracy, auc, ks, precision, recall, f1]\n            all_conf_matrices[name] = conf_matrix\n            print(f\"  {name}: F1={f1:.4f}, Recall={recall:.4f}, AUC={auc:.4f}\")\n        except Exception as e_base_eval:\n             print(f\"!!!!! Error evaluating model {name}: {e_base_eval} !!!!!\")\n             holdout_summary_nb.loc[name] = [np.nan] * 6\n             all_conf_matrices[name] = np.array([[np.nan]*2]*2)\n\n    end_base_time = time.time()\n    print(f\"\\nBase model evaluation finished in {end_base_time - start_base_time:.2f} seconds.\")\n\n\n    # --- 4. Evaluate Bagging Models on Test Set ---\n    print(\"\\n--- Evaluating Bagging Models on Hold-Out Test Set ---\")\n    bagging_models = {}\n    start_bagging_time = time.time()\n\n    # Use a subset of models for bagging as per the notebook's final list if needed\n    models_for_bagging = {k:v for k,v in models_nb_final.items() if k not in ['K Nearest Neighbors', 'Gaussian Naive Bayes']} # Example subset\n\n    for name, model in models_for_bagging.items():\n        print(f\"Processing Bagging: {name}\")\n        try:\n            bagging_model = BaggingClassifier(base_estimator=model, n_estimators=10, random_state=42, n_jobs=-1)\n            accuracy, auc, ks, conf_matrix, precision, recall, f1 = calculate_test_scores(bagging_model, X_trn_final, y_trn_final, X_tst_final, y_tst_final)\n            bagging_name = name + ' (Bagging)'\n            holdout_summary_nb.loc[bagging_name] = [accuracy, auc, ks, precision, recall, f1]\n            all_conf_matrices[bagging_name] = conf_matrix\n            print(f\"  {bagging_name}: F1={f1:.4f}, Recall={recall:.4f}, AUC={auc:.4f}\")\n        except Exception as e_bagging_eval:\n             print(f\"!!!!! Error evaluating Bagging model {name}: {e_bagging_eval} !!!!!\")\n             holdout_summary_nb.loc[name + ' (Bagging)'] = [np.nan] * 6\n             all_conf_matrices[name + ' (Bagging)'] = np.array([[np.nan]*2]*2)\n\n\n    end_bagging_time = time.time()\n    print(f\"\\nBagging model evaluation finished in {end_bagging_time - start_bagging_time:.2f} seconds.\")\n\n\n    # --- 5. Evaluate Stacking Model on Test Set ---\n    print(\"\\n--- Evaluating Stacking Model on Hold-Out Test Set ---\")\n    start_stacking_time = time.time()\n    # Use the same subset of models for stacking's first level\n    models_for_stacking = models_for_bagging # Using the same subset for consistency\n\n    print(\"Generating L1 predictions for stacking...\")\n    X2_trn_stack, X2_tst_stack = fit_first_level_preds(models_for_stacking, X_trn_final, y_trn_final, X_tst_final)\n\n    # Check for NaNs introduced by failed base models\n    if np.isnan(X2_trn_stack).any() or np.isnan(X2_tst_stack).any():\n        print(\"Warning: NaNs found in stacking features. Attempting imputation with 0.\")\n        X2_trn_stack = np.nan_to_num(X2_trn_stack, nan=0.0) # Replace NaN with 0\n        X2_tst_stack = np.nan_to_num(X2_tst_stack, nan=0.0)\n\n    # Define and evaluate the L2 model (XGBoost in the notebook)\n    stack_model_final = xgb.XGBClassifier(max_depth=3, random_state=42, use_label_encoder=False, eval_metric='logloss')\n    print(\"Evaluating final stacking model (XGBoost)...\")\n    try:\n        accuracy, auc, ks, conf_matrix, precision, recall, f1 = calculate_test_scores(stack_model_final, X2_trn_stack, y_trn_final, X2_tst_stack, y_tst_final)\n        stacking_name = 'Stacking Model (XGB)'\n        holdout_summary_nb.loc[stacking_name] = [accuracy, auc, ks, precision, recall, f1]\n        all_conf_matrices[stacking_name] = conf_matrix\n        print(f\"  {stacking_name}: F1={f1:.4f}, Recall={recall:.4f}, AUC={auc:.4f}\")\n    except Exception as e_stack_eval:\n         print(f\"!!!!! Error evaluating Stacking model: {e_stack_eval} !!!!!\")\n         holdout_summary_nb.loc['Stacking Model (XGB)'] = [np.nan] * 6\n         all_conf_matrices['Stacking Model (XGB)'] = np.array([[np.nan]*2]*2)\n\n\n    end_stacking_time = time.time()\n    print(f\"\\nStacking model evaluation finished in {end_stacking_time - start_stacking_time:.2f} seconds.\")\n\n\n    # --- 6. Display Final Summary ---\n    print(\"\\n--- Final Hold-Out Test Set Summary (Sorted by F1-Score) ---\")\n    final_summary_sorted = holdout_summary_nb.sort_values(by='f1', ascending=False)\n    print(final_summary_sorted)\n\n    # Optionally display confusion matrices\n    # print(\"\\n--- Confusion Matrices (Test Set) ---\")\n    # for name, matrix in all_conf_matrices.items():\n    #     print(f\"\\n{name}:\")\n    #     print(matrix)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-29T22:11:45.994308Z",
     "iopub.execute_input": "2025-10-29T22:11:45.995307Z",
     "iopub.status.idle": "2025-10-29T22:12:01.759118Z",
     "shell.execute_reply.started": "2025-10-29T22:11:45.995280Z",
     "shell.execute_reply": "2025-10-29T22:12:01.758334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "--- [Notebook Pipeline - Model 7] Part 9: Final Evaluation on Test Set ---\n\n--- Evaluating Base Models on Hold-Out Test Set ---\nProcessing: Logistic Regression\n  Logistic Regression: F1=0.7472, Recall=0.8538, AUC=0.9283\nProcessing: Decision Tree\n  Decision Tree: F1=0.7320, Recall=0.8834, AUC=0.9259\nProcessing: K Nearest Neighbors\n  K Nearest Neighbors: F1=0.6015, Recall=0.5224, AUC=0.7587\nProcessing: Random Forest\n  Random Forest: F1=0.7385, Recall=0.8812, AUC=0.9297\nProcessing: Gaussian Naive Bayes\n  Gaussian Naive Bayes: F1=0.6761, Recall=0.6416, AUC=0.9096\nProcessing: Light GBM\n  Light GBM: F1=0.7466, Recall=0.8713, AUC=0.9317\nProcessing: XGBoost\n  XGBoost: F1=0.7446, Recall=0.8740, AUC=0.9316\nProcessing: Gradient Boosting\n  Gradient Boosting: F1=0.7459, Recall=0.8713, AUC=0.9317\nProcessing: Neural Network\n  Neural Network: F1=0.7419, Recall=0.8812, AUC=0.9320\n\nBase model evaluation finished in 11.01 seconds.\n\n--- Evaluating Bagging Models on Hold-Out Test Set ---\nProcessing Bagging: Logistic Regression\n!!!!! Error evaluating Bagging model Logistic Regression: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: Decision Tree\n!!!!! Error evaluating Bagging model Decision Tree: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: Random Forest\n!!!!! Error evaluating Bagging model Random Forest: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: Light GBM\n!!!!! Error evaluating Bagging model Light GBM: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: XGBoost\n!!!!! Error evaluating Bagging model XGBoost: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: Gradient Boosting\n!!!!! Error evaluating Bagging model Gradient Boosting: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\nProcessing Bagging: Neural Network\n!!!!! Error evaluating Bagging model Neural Network: BaggingClassifier.__init__() got an unexpected keyword argument 'base_estimator' !!!!!\n\nBagging model evaluation finished in 0.00 seconds.\n\n--- Evaluating Stacking Model on Hold-Out Test Set ---\nGenerating L1 predictions for stacking...\n  Generating stacking preds for: Logistic Regression\n  Generating stacking preds for: Decision Tree\n  Generating stacking preds for: Random Forest\n  Generating stacking preds for: Light GBM\n  Generating stacking preds for: XGBoost\n  Generating stacking preds for: Gradient Boosting\n  Generating stacking preds for: Neural Network\nEvaluating final stacking model (XGBoost)...\n  Stacking Model (XGB): F1=0.7110, Recall=0.8683, AUC=0.9101\n\nStacking model evaluation finished in 4.72 seconds.\n\n--- Final Hold-Out Test Set Summary (Sorted by F1-Score) ---\n                               accuracy       auc        ks  precision  \\\nLogistic Regression            0.873703  0.928328  0.737294   0.664206   \nLight GBM                      0.870716  0.931731  0.742526   0.653102   \nGradient Boosting              0.870218  0.931696  0.742798   0.651989   \nXGBoost                        0.868974  0.931578  0.742165   0.648633   \nNeural Network                 0.865986  0.931999  0.743775   0.640629   \nRandom Forest                  0.863580  0.929715  0.740731   0.635542   \nDecision Tree                  0.858601  0.925929  0.737044   0.624866   \nStacking Model (XGB)           0.845739  0.910065  0.709025   0.602001   \nGaussian Naive Bayes           0.865654  0.909559  0.682385   0.714588   \nK Nearest Neighbors            0.848726  0.758710  0.494604   0.708913   \nLogistic Regression (Bagging)       NaN       NaN       NaN        NaN   \nDecision Tree (Bagging)             NaN       NaN       NaN        NaN   \nRandom Forest (Bagging)             NaN       NaN       NaN        NaN   \nLight GBM (Bagging)                 NaN       NaN       NaN        NaN   \nXGBoost (Bagging)                   NaN       NaN       NaN        NaN   \nGradient Boosting (Bagging)         NaN       NaN       NaN        NaN   \nNeural Network (Bagging)            NaN       NaN       NaN        NaN   \n\n                                 recall        f1  \nLogistic Regression            0.853834  0.747176  \nLight GBM                      0.871298  0.746584  \nGradient Boosting              0.871298  0.745856  \nXGBoost                        0.873956  0.744622  \nNeural Network                 0.881169  0.741889  \nRandom Forest                  0.881169  0.738466  \nDecision Tree                  0.883447  0.731991  \nStacking Model (XGB)           0.868261  0.711021  \nGaussian Naive Bayes           0.641610  0.676135  \nK Nearest Neighbors            0.522399  0.601530  \nLogistic Regression (Bagging)       NaN       NaN  \nDecision Tree (Bagging)             NaN       NaN  \nRandom Forest (Bagging)             NaN       NaN  \nLight GBM (Bagging)                 NaN       NaN  \nXGBoost (Bagging)                   NaN       NaN  \nGradient Boosting (Bagging)         NaN       NaN  \nNeural Network (Bagging)            NaN       NaN  \n",
     "output_type": "stream"
    }
   ],
   "execution_count": 13
  }
 ]
}
